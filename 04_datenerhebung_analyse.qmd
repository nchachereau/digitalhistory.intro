---
title: Datenerhebung und -aufbereitung
suppress-bibliography: true
lang: de
editor: 
  markdown: 
    wrap: sentence
---

Jede Art von Forschung ist auf Daten angewiesen, seien sie mittels Personenbefragungen, medizinischer Messungen, Webscraping oder interpretierender Analysen von Texten erhoben. 
Auf Grundlage von Daten können Forschungsfragen beantwortet, Thesen aufgestellt, Behauptungen widerlegt, Narrative untermauert werden.
Analysen, die sich mit einem kleinen Set von Quellen bzw. Daten befassen, präsentieren Ergebnisse dabei oft in Form von Synthesen, die sich aus einer vorangehenden Interpretation der zugrundeliegenden Dokumente ergeben.
Über das Quellenverzeichnis und entsprechende Anmerkungen im Text wird die Grundlage nachvollziehbar;
dass ein bestimmter Abschnitt, ein Satz oder ein Wort auf eine gewisse Weise ausgelegt werden, wird aber auch durch die jeweiligen Forscher:innen selbst beeinflusst -- 
eine Literaturwissenschaftlerin beispielsweise, die über Männerfiguren bei Joanne K. Rowling promoviert hat, wird bei der Diskussion um deren mögliche Autorschaft von *The Cuckoo’s Calling* (siehe @sec-digitaletools) diesen Text anders lesen und andere Argumente dafür oder dagegen aufwerfen als ein langjähriger Harry-Potter-Fan mit viel Leseerfahrung, aber anderer bzw. weniger formaler Ausbildung.
Beide werden fundierte Aussagen treffen und Begründungen geben können, ob und wieso *The Cuckoo’s Calling* von Rowling verfasst wurde oder nicht;
beide werden auf ihre Erfahrung und gründliche Auseinandersetzung mit Rowlings Werk verweisen;
und beide werden mit einzelnen Sätzen oder Passagen für eine Sichtweise argumentieren, die von einer dritten Person genau gegenteilig genutzt würde.
Die Datengrundlage ist also dieselbe und nachvollziehbar, die Auswertung bzw. die Auswertungsstrategien hingegen sind es nicht mehr, und somit auch nicht die daraus gewonnenen Ergebnisse, die ja auch wieder Forschungsdaten darstellen.

Computergestützte Analysen haben den Anspruch, in allen Schritten nachvollziehbar zu sein und dadurch auch nachnutzbare Daten zu produzieren:
Nicht nur die Quellengrundlage, also die Erhebung von Daten und die Erstellung eines Datensatzes, sondern auch alle Schritte von der Datenanreicherung und -verfeinerung über die genutzten Methoden bzw. Programme für die Auswertung bis hin zur Sicherung und Aufbewahrung sollen transparent, gut dokumentiert und nachvollziebar sein.
Zum einen, um die Resultate und die darauf fußenden Aussagen belastbar zu machen;
zum anderen, um die gewonnenen Daten zur weiteren Nutzung kostenfrei und offen verfügbar zu machen.
Zu den Prinzipien, die bei der Arbeit mit Daten berücksichtigt werden sollten, geht es nochmals in [Kapitel 5](05_FAIR_CARE.qmd). 
An dieser Stelle geht es um die konkreten Arbeitsschritte bei der Datenerhebung und Datenaufbereitung, die in dh-Projekten häufig vorkommen.


## Datenerhebung

Es gibt verschiedene Möglichkeiten, Daten für die historische Forschung zu erheben bzw. zu erstellen, von denen einige im Folgenden kurz angesprochen werden.

Für Zeiträume, in denen Quellen vergleichsweise knapp sind und keine seriellen Daten existieren, bietet sich die **Digitalisierung von Texten** und deren anschließende Analyse an. 
Digitalisierung beinhaltet dabei nicht nur die Transformation von einer physischen Quelle in ein digitales Bild, sondern auch die Anreicherung des Bilds mit Layout und Text: 
Erst durch eine Markierung von Bereichen, in denen Text vorkommt, ist es in einem zweiten Schritt möglich, diesen als solchen zu erkennen und damit maschinenlesbar und auswertbar zu machen. 
Eine solche Umwandlung vom Bild zum Text ist dabei sowohl für moderne Texte, die als Typoskript vorliegen, als auch für vormoderne Handschriften und Drucke möglich, in lateinischer ebenso wie in arabischer, chinesischer oder japanischer Schrift. 
Es gibt kostenpflichtige Programme wie den [Abbyy FineReader](https://pdf.abbyy.com/), aber auch Open-Source-Tools mit und ohne Graphical User Interface (GUI).
Weit verbreitet ist [Transkribus](https://transkribus.eu), das viele Funktionalitäten bündelt; 
die Texterkennung ist ab einer gewissen Menge Seiten allerdings kostenpflichtig, wobei studentische Projekte auf Anfrage unterstützt werden können.
Programme, die über die Kommandozeile laufenm gänzlich kostenfrei sind und ebenfalls zahlreiche Funktionalitäten bieten, sind beispielsweise [Kraken](https://kraken.re/master/index.html), [OCR4all](https://www.ocr4all.org/), [OCRopus](https://github.com/ocropus/ocropy/wiki) oder [Calamari](https://github.com/Calamari-OCR/calamari).

Zur **Extraktion von Daten** aus digitalen/digitalisierten Texten existieren verschiedene Möglichkeiten mithilfe kleiner Kommandozeilenprogramme (eher mühsam und schwierig zu lesen) oder mit Packages für Programmiersprachen, für die Geisteswissenschaften vor allem R oder Python (siehe dazu auch @sec-digitaletools).
So können besipielsweise aus digitalisierten Telefonbüchern Entitäten wie Personen, Straßennamen oder Berufe oder aus alten Theaterprogrammheften gespielte Stücke, beteiligte Schauspieler:innen und verantwortliche Regisseurinnen extrahiert und als Datensätze weitergenutzt werden.[^1]

[^1]: Ein gut nachvollziehbares Tutorial zur Extraktion von Daten aus Telefonbüchern hat [Lindsey Wieck](https://lindseywieck.com/) für einen dh-Kurs an der St. Mary’s University in San Antonio erstellt: [https://lindseywieck.com/fall_2016_sf/gatheringdatatutorial.html](https://lindseywieck.com/fall_2016_sf/gatheringdatatutorial.html). 
[Derek Miller](http://derek.visualizingbroadway.com/about.html) arbeitet zu Broadway-Vorstellungen, [Visualizing Broadway](https://visualizingbroadway.com/index.html), ein Projekt, das [hier](https://digitalhumanities.fas.harvard.edu/100-years-of-broadway-shows-at-once/) beschrieben wird; [hier](https://www.youtube.com/watch?v=KUTPX2Ohcqs) gibt es dazu ein Video in Vorlesungslänge.

Der anfängliche Aufwand, der einer automatisierten Datenextraktion vorangeht und die steile Lernkurve mancher Programme können abschreckend wirken.
Und wenn Sie nur ein Theaterprogramm detaillierter auswerten wollen, sind Sie sicher schneller, wenn Sie die entsprechenden Daten in eine Tabellensoftware abtippen.
Wenn Sie aber einen größeren Quellenbestand zur Verfügung haben, der in sich ähnlich strukturiert ist, wie das bei Telefonbüchern oder Theaterprogrammheften der Fall sein dürfte, macht es kaum einen Unterschied mehr, ob Sie 10 oder 1000 Theaterprogramme analysieren möchten.
Zudem können Sie Ihr erstelltes Skript, Ihr kleines Computerprogramm, anderen zur Verfügung stellen oder für ähnlich strukturierte Quellen in einem anderen Projekt nachnutzen.

Wenn Sie mit bereits digitalisierten Beständen aus öffentlichen Institutionen wie Galerien, Bibliotheken, Museen oder Archiven arbeiten wollen (sog. [GLAM](https://de.wikipedia.org/wiki/GLAM)s: **G**alleries,  **L**ibraries, **A**rchives, **M**useums), besteht oft die Möglichkeit, Daten über **Schnittstellen** herunterzuladen.[^2]
Solche [API](https://en.wikipedia.org/wiki/API)s (**A**pplication **p**rogramming **i**nterface) ermöglichen eine Kommunikation zwischen zwei Computern, ohne dass hierfür der Umweg über ein egraphische Oberfläche nötig ist.
Anstatt also beispielsweise über die Suchmaske der [Staatlichen Museen zu Berlin](https://smb.museum-digital.de/home?navlang=de) nach Objekten oder Dokumenten mithilfe verschiedener Schlagwörter zu suchen und die Ergebnisse dann einzeln herunterzuladen, kann Ihr Computer mit der Schnittstelle des Museums direkt kommunizieren und mit einfachen Befehlen ganze Ergebnislisten zur Weiterarbeit herunterladen. 
Für solche Abfragen können ein Kommandozeilenprogramm oder Programmiersprachen genutzt werden, die Abfrage besteht dabei im Wesentlichen aus einer Zeile, wie hier in der Programmiersprache R:

`library(jsonlite)` 

`cats <- fromJSON("https://smb.museum-digital.de/json/objects?&s=katze")`

Das Ergebnis der Suchanfrage nach "katze" wird in der Variable `cats` gespeichert, und diese kann zur Weiterarbeit in ein Tabellenformat exportiert werden:

`write.csv(cats, "docs/cats_smb.csv")`

Die Funktion `write.csv` speichert den Inhalt der Variable `cats` als csv-Datei[^3] unter dem Dateipfad "docs/cats_smb.csv" auf der Festplatte.

![Beginn der Trefferliste für "katze" ¨über die API der Staatlichen Museen zu Berlin](images/cats_smb.png)



[^2]: Unter [openglam.ch](openglam.ch) finden sich Informationen zu Schweizer GLAM-Einrichtungen, die offene Daten anbieten.

[^3]: **c**omma **s**eparated **v**alue ist ein Format, in dem einzelne Werte, values, über spezifische Trenner, meist Kommas, eindeutig abgrenzbar sind und somit in einem Tabellenformat angezeigt werden können, wobei jeder Wert in einer separaten Zelle steht.

Wenn Webseiten keine Schnittstellen zur Verfügung stellen, besteht die Möglichkeit, mit **Web Scraping** an gewünschte Daten zu kommen. 
Je nach Webseite bzw. Inhalten ist die Rechtslage nicht ganz klar;
zum Download von Webseiten mit der Programmiersprache Python von William J. Turkel und Adam Crymble gibt es [eine Lektion im Programming Historian](https://programminghistorian.org/en/lessons/working-with-web-pages).
Ein weiteres Tutorial zur Datenakquise, von Zach Coble, Liz Rodrigues, Erin Pappas, Chelcie Rowell, und Yasmeen Shorish, findet sich [hier](https://dlfteach.pubpub.org/pub/collecting-data-web-scraping/release/1).

## Datenaufbereitung

OpenRefine


## Datenanalyse



## Datensicherung

dokumentieren

    Wie wurden die Daten erhoben?
    Wie wurden die Daten annotiert?
    In welchem Format sind die Daten vorhanden?
    Welche Software wurde benutzt?
    Was stellen die Daten dar?

Siehe auch [Kapitel 5](05_FAIR_CARE.qmd) zu Fragen von nachhaltiger Speicherung.