{"title":"Datenerhebung, -aufbereitung und -analyse","markdown":{"yaml":{"title":"Datenerhebung, -aufbereitung und -analyse","suppress-bibliography":true,"lang":"de","editor":{"markdown":{"wrap":"sentence"}}},"headingText":"Datenerhebung","containsRefs":false,"markdown":"\n\nJede Art von Forschung ist auf Daten angewiesen, seien sie mittels Personenbefragungen, medizinischer Messungen, Web Scraping oder interpretierender Analysen von Texten erhoben. \nAuf Grundlage von Daten können Forschungsfragen beantwortet, Thesen aufgestellt, Behauptungen widerlegt, Narrative untermauert werden.\nAnalysen, die sich mit einem kleinen Set von Quellen bzw. Daten befassen, präsentieren Ergebnisse dabei oft in Form von Synthesen, die sich aus einer vorangehenden Interpretation der zugrundeliegenden Dokumente ergeben.\nÜber das Quellenverzeichnis und entsprechende Anmerkungen im Text wird die Grundlage nachvollziehbar;\ndass ein bestimmter Abschnitt, ein Satz oder ein Wort auf eine gewisse Weise ausgelegt werden, wird aber auch durch die jeweiligen Forscher:innen selbst beeinflusst -- \neine Literaturwissenschaftlerin beispielsweise, die über Männerfiguren bei Joanne K. Rowling promoviert hat, wird bei der Diskussion um deren mögliche Autorschaft von *The Cuckoo’s Calling* (siehe @sec-digitaletools) diesen Text anders lesen und andere Argumente dafür oder dagegen aufwerfen als ein langjähriger Harry-Potter-Fan mit viel Leseerfahrung, aber anderer bzw. weniger formaler Ausbildung.\nBeide werden fundierte Aussagen treffen und Begründungen geben können, ob und wieso *The Cuckoo’s Calling* von Rowling verfasst wurde oder nicht;\nbeide werden auf ihre Erfahrung und gründliche Auseinandersetzung mit Rowlings Werk verweisen;\nund beide werden mit einzelnen Sätzen oder Passagen für eine Sichtweise argumentieren, die von einer dritten Person genau gegenteilig genutzt würde.\nDie Datengrundlage ist also dieselbe und nachvollziehbar, die Auswertung bzw. die Auswertungsstrategien hingegen sind es nicht mehr, und somit auch nicht die daraus gewonnenen Ergebnisse, die ja auch wieder Forschungsdaten darstellen.\n\nComputergestützte Analysen haben den Anspruch, in allen Schritten nachvollziehbar zu sein und dadurch auch nachnutzbare Daten zu produzieren:\nNicht nur die Quellengrundlage, also die Erhebung von Daten und die Erstellung eines Datensatzes, sondern auch alle Schritte von der Datenanreicherung und -verfeinerung über die genutzten Methoden bzw. Programme für die Auswertung bis hin zur Sicherung und Aufbewahrung sollen transparent, gut dokumentiert und nachvollziehbar sein.\nZum einen, um die Resultate und die darauf fußenden Aussagen belastbar zu machen;\nzum anderen, um die gewonnenen Daten zur weiteren Nutzung kostenfrei und offen verfügbar zu machen.\nZu den Prinzipien, die bei der Arbeit mit Daten berücksichtigt werden sollten, geht es nochmals in [Kapitel 5](05_FAIR_CARE.qmd). \nAn dieser Stelle stehen die konkreten Arbeitsschritte bei der Datenerhebung und -aufbereitung, der Datenanalyse und -sicherung im Zentrum, die in Digital-History-Projekten häufig vorkommen.\n\n\n\nEs gibt verschiedene Möglichkeiten, Daten für die historische Forschung zu erheben bzw. zu erstellen, von denen einige im Folgenden kurz angesprochen werden.\n\nFür Zeiträume, in denen Quellen vergleichsweise knapp sind und keine seriellen Daten existieren, bietet sich die **Digitalisierung von Texten** und deren anschließende Analyse an. \nDigitalisierung beinhaltet dabei nicht nur die Transformation von einer physischen Quelle in ein digitales Bild, sondern auch die Anreicherung des Bilds mit Layout und Text: \nErst durch eine Markierung von Bereichen, in denen Text vorkommt, ist es in einem zweiten Schritt möglich, diesen als solchen zu erkennen und damit maschinenlesbar und auswertbar zu machen. \nEine solche Umwandlung vom Bild zum Text ist dabei sowohl für moderne Texte, die als Typoskript vorliegen, als auch für vormoderne Handschriften und Drucke möglich, in lateinischer ebenso wie in arabischer, chinesischer oder japanischer Schrift. \nEs gibt kostenpflichtige Programme wie den [Abbyy FineReader](https://pdf.abbyy.com/), aber auch Open-Source-Tools mit und ohne Graphical User Interface (GUI).\nWeit verbreitet ist [Transkribus](https://transkribus.eu), das viele Funktionalitäten bündelt; \ndie Texterkennung ist ab einer gewissen Menge Seiten allerdings kostenpflichtig, wobei studentische Projekte auf Anfrage unterstützt werden können.\nProgramme, die über die Kommandozeile laufen, gänzlich kostenfrei sind und ebenfalls zahlreiche Funktionalitäten bieten, sind beispielsweise [Kraken](https://kraken.re/master/index.html), [OCR4all](https://www.ocr4all.org/), [OCRopus](https://github.com/ocropus/ocropy/wiki) oder [Calamari](https://github.com/Calamari-OCR/calamari).\n\nZur **Extraktion von Daten** aus digitalen/digitalisierten Texten existieren verschiedene Möglichkeiten mithilfe kleiner Kommandozeilenprogramme (eher mühsam und schwierig zu lesen) oder mit Packages für Programmiersprachen, für die Geisteswissenschaften vor allem R oder Python (siehe dazu auch @sec-digitaletools).\nSo können beispielsweise aus digitalisierten Telefonbüchern Entitäten, also Einheiten, wie Personen, Straßennamen oder Berufe oder aus alten Theaterprogrammheften gespielte Stücke, beteiligte Schauspieler:innen und verantwortliche Regisseurinnen extrahiert und als Datensätze weitergenutzt werden.^[Ein gut nachvollziehbares Tutorial zur Extraktion von Daten aus Telefonbüchern hat [Lindsey Wieck](https://lindseywieck.com/) für einen DH-Kurs an der St. Mary’s University in San Antonio erstellt: [https://lindseywieck.com/fall_2016_sf/gatheringdatatutorial.html](https://lindseywieck.com/fall_2016_sf/gatheringdatatutorial.html). [Derek Miller](http://derek.visualizingbroadway.com/about.html) arbeitet zu Broadway-Vorstellungen, [Visualizing Broadway](https://visualizingbroadway.com/index.html), ein Projekt, das [hier](https://digitalhumanities.fas.harvard.edu/100-years-of-broadway-shows-at-once/) beschrieben wird; [hier](https://www.youtube.com/watch?v=KUTPX2Ohcqs) gibt es dazu ein Video in Vorlesungslänge.]\n\nDer anfängliche Aufwand, der einer automatisierten Datenextraktion vorangeht und die steile Lernkurve bei der Bedienung mancher Programme können abschreckend wirken.\nWenn Sie nur ein Theaterprogramm detaillierter auswerten wollen, sind Sie sicher schneller, wenn Sie die entsprechenden Daten in eine Tabellensoftware abtippen.\nWenn Sie aber einen größeren Quellenbestand zur Verfügung haben, der in sich ähnlich strukturiert ist, wie das bei Telefonbüchern oder einer Serie von Theaterprogrammheften der Fall sein dürfte, macht es kaum einen Unterschied mehr, ob Sie zehn oder hundert Theaterprogramme analysieren möchten.\nZudem können Sie Ihr erstelltes Skript, Ihr kleines Computerprogramm, anderen zur Verfügung stellen oder für ähnlich strukturierte Quellen in einem anderen Projekt nachnutzen.\n\nWenn Sie mit bereits digitalisierten Beständen aus öffentlichen Institutionen wie Galerien, Bibliotheken, Museen oder Archiven arbeiten wollen (sog. [GLAM](https://de.wikipedia.org/wiki/GLAM)s: **G**alleries,  **L**ibraries, **A**rchives, **M**useums), besteht oft die Möglichkeit, Daten über **Schnittstellen** herunterzuladen.^[Unter [openglam.ch](https://openglam.ch) finden sich Informationen zu Schweizer GLAM-Einrichtungen, die offene Daten anbieten.]\nSolche Schnittstellen, engl. [API](https://en.wikipedia.org/wiki/API) (**A**pplication **P**rogramming **I**nterface), ermöglichen eine Kommunikation zwischen zwei Computern, ohne dass hierfür der Umweg über eine graphische Oberfläche nötig ist.\nAnstatt also beispielsweise über die Suchmaske der [Staatlichen Museen zu Berlin](https://smb.museum-digital.de/home?navlang=de) nach Objekten oder Dokumenten mithilfe verschiedener Schlagwörter zu suchen und die Ergebnisse dann einzeln herunterzuladen, kann Ihr Computer mit der Schnittstelle des Museums direkt kommunizieren und mit einfachen Befehlen ganze Ergebnislisten zur Weiterarbeit herunterladen. \nFür solche Abfragen können ein Kommandozeilenprogramm oder Programmiersprachen genutzt werden, die Abfrage besteht dabei im Wesentlichen aus einer Zeile, wie hier in der Programmiersprache R:\n\n`library(jsonlite)` \n\n`cats <- fromJSON(\"https://smb.museum-digital.de/json/objects?&s=katze\")`\n\n> Wenn Sie die Schritte nachvollziehen möchten, können Sie R [hier](https://www.r-project.org/) herunterladen. Wenn Sie das Programm öffnen, müssen Sie zuerst das Paket `jsonlite` installieren:\n>`install.packages(\"jsonlite\")`\\\nMit \"Enter\" wird das Paket installiert.\\ \nDann können Sie die zwei Zeilen oben eintippen und ebenfalls mit \"Enter\" ausführen. Die Ergebnisse Ihrer Suche können Sie sich mit\\\n`cats` + \"Enter\" \\\nanzeigen lassen.\n\nDas Ergebnis der Suchanfrage nach \"katze\" wird in der Variable `cats` gespeichert, und diese kann zur Weiterarbeit in ein Tabellenformat exportiert werden:\n\n`write.csv(cats, \"docs/cats_smb.csv\")`\n\nDie Funktion `write.csv` speichert den Inhalt der Variable `cats` als csv-Datei^[**c**omma **s**eparated **v**alue ist ein Format, in dem einzelne Werte, *values*, über spezifische Trenner, meist *commas*, eindeutig abgrenzbar sind und somit in einem Tabellenformat angezeigt werden können, wobei jeder Wert in einer separaten Zelle steht. Tabellensoftware wie Excel, Google Sheets oder Numbers kann csv-Dateien öffnen.] unter dem Dateipfad \"docs/cats_smb.csv\" auf der Festplatte.\n\n![Beginn der Trefferliste für \"katze\" über die API der Staatlichen Museen zu Berlin](images/cats_smb.png)\n\n>Um Abfragen zu vermeiden, die die Server überlasten, haben die meisten APIs entweder eine Authentifizierung oder eine maximale Trefferanzahl pro Abfrage eingebaut.\nBeim obigen Beispiel erhalten Sie dadurch nicht die gesamte Trefferanzahl (134, aus der Spalte \"total\" ersichtlich), sondern nur die ersten 24 -- diese Einstellungen haben die Entwickler:innen der Schnittstelle gemacht.\nUm dennoch alle Treffer mit einer Abfrage zu erhalten, müssten Sie die Dokumentation der API lesen und die Abfrage etwas modifizieren. \n\nWenn Sie das interessiert, finden Sie Details in der Fußnote.[^longnote]\n\n[^longnote]: Die API aus dem Beispiel ist so konfiguriert, dass bei Abfragen mit Ergebnissen über 24 Treffern nur die ersten 24 ausgegeben werden; \ndas ist etwas ungewöhnlich, aber wir können damit umgehen, indem wir die maximale Trefferausgabe pro Anfrage auf 10 setzen -- diese Zahl ist nicht zu hoch, und wir können gut damit rechnen. \nDer Parameter für die maximale Trefferzahl kann mit `&breitenat=10` eingestellt werden.\nDen Startpunkt der Ausgabe kann man mit dem Parameter `&startwert=` ändern. \nUm also alle Treffer für eine Abfrage zu erhalten, können wir die Ergebnisse in 10er-Schritten abfragen und anschließend zusammenfügen.\nDamit das nicht zu einer copy-paste-Aktion wird, müssen wir etwas ausführlicher formulieren bzw. mehrere Variablen verwenden.\nDas hat aber den Vorteil, dass man auf diese Art dann nach jedem Begriff suchen kann.\\\n``` R\nbase_URL <- \"https://smb.museum-digital.de/json/objects?&s=katze\"\ncats <- fromJSON(base_URL)\nstart <-  0\nbreite <- 10\niterations <- cats$total[1]%/%10 + 1\nendsize <- cats$total[1]-(iterations-1) * 10\ncat_list <- data.frame()\nfor (i in 1:iterations){\n  if(i < iterations){\n    cat_list <- rbind(cat_list, fromJSON(paste(base_URL, \"&gbreitenat=10&startwert=\", start , sep=\"\")))\n  } else {\n    cat_list <- rbind(cat_list,fromJSON(paste(base_URL, \"&gbreitenat=\", \n                                                  endsize, \"&startwert=\", start, sep=\"\")))\n  }\n  start <- start + 10\n  write.csv(cat_list, \"Desktop/cat_list.csv\")\n}\n````\nZuerst machen wir den Code übersichtlicher und speichern den Großteil der URL in `base_URL`:\\\n`base_URL <- \"https://smb.museum-digital.de/json/objects?&s=katze\"`\\\nDie Ergebnisse der Suchanfrage werden wieder im Objekt `cats` gespeichert:\\\n`cats <- fromJSON(base_URL)`\\\nDie Anzahl Durchgänge für eine Abfrage ergibt sich aus der `Anzahl der totalen Treffer/10 + 1`; die Anzahl der Treffer lässt sich aus der Spalte \"total\" im Objekt `cats` entnehmen. In R formuliert man das so:\\\n`cats$total[1]`\\\nIm Katzenbeispiel sind es 134 Gesamttreffer, also `(134/10 ohne Rest)+1`, also `14` Durchgänge:\\\n`iterations <- cats$total[1]%/%10 + 1`\\\nDann setzt man den Startwert auf 0:\\\n`start <-  0`\\\nUnd die Maximaltreffer auf 10:\\\n`breite <- 10`\\\nDie letzte Iteration muss dabei nicht die nächsten 10 Treffer abfragen, sondern nur noch 4 (die letzten 4 nach 130):\\\n`endsize <- cats$total[1]-(iterations-1) * 10`\\\nDann erstellen wir eine leere Tabelle, einen data frame, die wir mit unseren Anfragen nach und nach befüllen. (Bei kleinen Datenmengen kann die Funktion `rbind` zur Verbindung von Einzeltabellen genutzt werden; bei größeren Datenmengen ist das iterative Verlängern von data frames nicht empfohlen):\\\n`cat_list <- data.frame()`\\\nWenn wir diese Variablen festgelegt haben, können wir einen Loop, eine Schleife bauen, die unter bestimmten Bedingungen verschiedene Aktionen ausführt:\\\n`for (i in 1:iterations){`\\\nFalls die letzte Iteration noch nicht erreicht ist, wird die Abfrage in 10er-Schritten durchgeführt, wobei der Startwert bei jedem Durchgang um 10 verschoben wird und die Ergebnisse hintereinander in `cat_list` geschrieben werden.\\\n  `if(i < iterations){`\\\n    `cat_list <- rbind(cat_list, fromJSON(paste(base_URL, \"&gbreitenat=10&startwert=\", start , sep=\"\")))`\\\n  ` } else {`\\\n  Sobald die letzte Iteration erreicht ist, werden nicht mehr die nächsten 10, sondern so viel Treffer, wie in `endsize` gespeichert, abgefragt, in unserem Beispiel 4:\\\n  `cat_list <- rbind(cat_list,fromJSON(paste(base_URL, \"&gbreitenat=\",`\\ \n                                                  `endsize, \"&startwert=\", start, sep=\"\")))`\\\n  `}`\\\n  `start <- start + 10`\\\n  Zum Schluss, in diesem Fall nach 14 Iterationen, wird die Tabelle in eine Datei geschrieben:\\\n  `write.csv(cat_list, \"Desktop/cat_list.csv\")`\\\n`}`\n\nWenn Webseiten keine Schnittstellen zur Verfügung stellen, besteht die Möglichkeit, mit **Web Scraping** an gewünschte Daten zu kommen. Je nach Webseite bzw. Inhalten ist die Rechtslage allerdings nicht ganz klar.\nZum Download von Webseiten mit der Programmiersprache Python gibt es [eine Lektion im Programming Historian](https://programminghistorian.org/en/lessons/working-with-web-pages) von William J. Turkel und Adam Crymble.\nEin weiteres Tutorial zur Datenakquise, von Zach Coble, Liz Rodrigues, Erin Pappas, Chelcie Rowell, und Yasmeen Shorish, findet sich [hier](https://dlfteach.pubpub.org/pub/collecting-data-web-scraping/release/1).\n\n## Datenaufbereitung^[Eine häufige Aussage ist, zur Datenvorbereitung/Preprocessing würde 80% der Arbeitszeit verwendet, zur Analyse und Interpretation blieben nur 20%. In einem [Blogartikel](https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/) von 2020 geht Leigh Dodds diesen Zahlen nach -- ganz so dramatisch ist das Verhältnis in Wahrheit wohl nicht.]\n\nBei der Arbeit mit Datensätzen, seien sie selbst erhoben oder von Dritten übernommen, ist es häufig der Fall, dass Informationen fehlen oder uneinheitlich erhoben wurden, was eine spätere Analyse erschwert.\n\nWenn in einer Umfrage unter Studierenden das Studienfach mit in eine Tabelle aufgenommen wurde, ohne zuvor Werte für diese Kategorie zu definieren, finden sich für \"Geschichte\" und \"Deutsch\" vielleicht auch folgende Varianten:\n\"Gesch.\", \"Geschichtswissenschaft\", \"Geschichtswissenschaften\", \"Geschihcte\", \"Germanistik\", \"Dt.\", \"Germ.\". \nAnstatt zwei Werten für zwei Studienfächer gibt es neun -- ohne, dass sich das Fächerspektrum erweitert hätte. \nIm besten Fall werden solche Varianten schon bei der Erhebung der Daten vermieden, indem eine feste Liste an Werten erstellt wird.\nErhält man jedoch einen Datensatz mit verschiedenen Varianten für ein und denselben Wert, muss man diese zusammenführen, um eine saubere Datengrundlage zu erhalten.\nSie können entweder mit `Strg-R` versuchen, verschiedene Schreibweisen zu finden und zu ersetzen; \nin Tabellenprogrammen wie Excel, Open Office oder Google Sheets können Sie sich einzigartige Werte einzelner Spalten anzeigen lassen und zusammengehörende Varianten zu einem Grundwert zusammenführen;\nam hilfreichsten, recht voraussetzungslos zu bedienen und dabei auch für große Datensätze nutzbar ist die Software [OpenRefine](https://openrefine.org/),  mit der Sie Daten extrahieren,^[Evan Peter Williamson: Fetching and Parsing Data from the Web with OpenRefine, Programming Historian 6 (2017), [https://doi.org/10.46430/phen0065]((https://doi.org/10.46430/phen0065.https://programminghistorian.org/en/lessons/fetch-and-parse-data-with-openrefine)).] säubern/vereinheitlichen^[Seth van Hooland, Ruben Verborgh, Max De Wilde: Cleaning Data with OpenRefine, Programming Historian 2 (2013), [https://doi.org/10.46430/phen0023](https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine).] und anreichern^[Karen Li-Lun Hwang: Enriching Reconciled Data with OpenRefine, The Bytegeist Blog 2018, [https://medium.com/the-bytegeist-blog/enriching-reconciled-data-with-openrefine-89b885dcadbb](https://medium.com/the-bytegeist-blog/enriching-reconciled-data-with-openrefine-89b885dcadbb)] können, um eine für Ihre Forschungsfrage und dafür notwendige Analysen sinnvolle Datengrundlage zu erhalten.\n\nFür Textdaten sind verschiedene Schritte zur Aufbereitung notwendig, je nachdem, welche Methode bzw. Software Sie nutzen möchten. \nFür die meisten Analysen ist es sinnvoll, mit sogenannten Stopword-Listen zu arbeiten.\n[Stopwords](https://en.wikipedia.org/wiki/Stop_word) sind Wörter, die vor einer Analyse aus einem Korpus entfernt werden, um aussagekräftigere Ergebnisse zu erhalten, gerade, wenn es um rein quantitative Methoden zur inhaltlichen Erschließung geht.\nStopwords sind Wörter mit grammatikalischen Funktionen, die in großer Zahl in Dokumenten vorkommen, jedoch wenig Bedeutung tragen.\nWenn man den unbearbeiteten Text dieses Guides nach Worthäufigkeiten auswertet, hier mit [Voyant-Tools](https://voyant-tools.org/) lässt sich nur schwerlich erahnen, worum es geht -- \"digital\" steht auf Platz 12, viel häufiger sind Artikel und Präpositionen.\nMit Hilfe einer Stopword-Liste, die die häufigsten nicht-sinntragenden Wörter aus dem Text entfernt, wird der Inhalt klarer: \n\n::: {layout=\"[46,-8,46]\"}\n![Worthäufigkeiten roher Text](images/word_frequencies_raw.png)\n\n![Worthäufigkeiten ohne Stopwords](images/word_frequencies_stopwords.png)\n:::\n\nWeitere Schritte beinhalten oft eine [Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung), also die Segmentierung in Einheiten der Wortebene, und eine [Lemmatisierung](https://de.wikipedia.org/wiki/Lemma_(Lexikographie)#Lemmatisierung), also die Rückführung von verschiedenene Formen eines Worts auf eine Grundform -- aus \"ist\", \"war\" und \"sind\" wird \"sein\".\nWie bei den Schreibvarianten der Studienfächer haben die verschiedenen Flexionsformen für die meisten Forschungsfragen keinen Mehrwert und können zur weiteren Analyse zusammengeführt werden.\nFür solche vorbereitenden Schritte gibt es existierende Software und Packages für Programmiersprachen, sodass hier das Rad nicht neu erfunden werden muss, vor allem für moderne, weit verbreitete Sprachen, siehe auch @sec-digitaltools.\nSchwieriger wird es für nicht-standardisierte Sprachen bzw. Sprachformen, also dialektal geprägte oder vormoderne Texte. \nZwar gibt es auch hierfür Programme, die tatsächlich erreichte Präzision muss dabei jedoch je nach Quelle beurteilt werden.\n\n\n## Datenanalyse\n\nWenn Sie einen Datensatz zur Analyse zur Verfügung haben, aus selbst erhobenen Daten oder durch Nachnutzung eines vorhandenen, und für Ihre Zwecke aufbereitet haben, folgt (endlich) auch die Analyse.\nWelche Software oder Methoden Sie verwenden, hängt dabei nicht nur von der Art und Menge der Daten, sondern auch dem Datenformat und vor allem auch Ihrer Forschungsfrage ab.\nWenn Sie eine Personendatenbank haben, in der Briefschreiber:innen und Empfänger:innen aufgenommen sind und der Wohnort der Personen bekannt ist, Sie es jedoch versäumt haben, die Datierungen der Einzelbriefe zu verzeichnen, können Sie nur eine räumliche Verteilung, keine raum-zeitliche Entwicklung eines Briefschreiber:innennetzwerks darstellen.^[Ein Großprojekt an der Universität Stanford, [\"Mapping the Republic of Letters\"](http://republicofletters.stanford.edu/), hat für das 18. Jahrhundert das Briefnetzwerk europäischer Gelehrter modelliert. Ein Fallbeispiel ist das Netzwerk Voltaires, in verschiedenen Visualisierungen: [http://republicofletters.stanford.edu/publications/voltaire/letters/](http://republicofletters.stanford.edu/publications/voltaire/letters/). Dan Edelstein. Interactive Visualization for Voltaire’s Correspondence Network. Letters in Voltaire’s Network [Created using Palladio, http://hdlab.stanford.edu/palladio].] \nWenn Sie aber nur an der örtlichen Verteilung weiblicher und männlicher Verfasser:innen interessiert sind und die zeitliche Komponente für Sie keine Rolle spielt, erübrigt sich auch ein raum-zeitliche Analyse.\nBevor Sie sich also für eine Methode entscheiden, sollten Sie sich fragen, zu welchem Zweck Sie Ihren Datensatz nutzen wollen und  welche Frage(n) er beantworten soll. \n\nIn einem nächsten Schritt sollte über die konkrete Art der Analyse nachgedacht werden, die mit den vorhandenen Daten möglich ist.\nUnter den zahlreichen Möglichkeiten für die Arbeit mit **strukturellen Daten** sind für die Geschichtswissenschaften u.a. die [Netzwerkanalyse](https://de.wikipedia.org/wiki/Soziale_Netzwerkanalyse) oder die [Regressionsanalyse](https://de.wikipedia.org/wiki/Regressionsanalyse) häufig genutzte Methoden. \nFür **textuelle Daten** bieten sich ebenfalls verschiedene Arten der Analyse an, darunter beispielsweise Auszählungen von Worthäufigkeiten als Teil der [Stylometrie](https://de.wikipedia.org/wiki/Stilometrie)/Zuschreibung von Autor:innenschaft (siehe @sec-digitaletools), [Topic Modelling](https://en.wikipedia.org/wiki/Topic_model) als statistische Methode zur Identifizierung wiederkehrender Themen in größeren Textbeständen, oder [Sentimentanalyse](https://en.wikipedia.org/wiki/Sentiment_analysis), um Stimmungen, Gefühle, Bewertungen aus Textpassagen zu extrahieren.\nWenn Sie über **georeferenzierte Daten** verfügen, können Sie verschiedene Analysen mithilfe von [GIS](https://guides.temple.edu/gisfords) (Geographic Information System) durchführen und visualisieren.\n\nOb Sie für Topic Modelling ein eigenes Skript schreiben oder vorhandene Software nutzen, ob Sie Regressionsanalysen selbst durchführen oder auf Webseiten durchführen lassen, ist dabei Ihre Entscheidung;\noftmals ist das Nutzen vorhandener Webangebote für erste kurze Analysen sinnvoll, um zu überlegen, ob die vorgesehene Methode überhaupt sinnvolle Ergebnisse  liefern kann.\nFür größere Projekte, in denen komplexere Analysen über einen längeren Zeitraum durchgeführt werden sollen, bietet sich die Arbeit mit Programmiersprachen schon allein deswegen an, weil so ein sehr hohes Maß an Anpassungen von vorhandenen Funktionen für die eigenen Zwecke und die völlige Kontrolle über die eigenen Daten ermöglicht wird.\nEine Auflistung häufig genutzter Tools für die historische Arbeit findet sich in @sec-digitaltools.\n\n\n## Datensicherung {#sec-sicherung}\n\nIn [Kapitel 5](05_FAIR_CARE.qmd) wird es um Fragen zur nachhaltigen Speicherung von Forschungsdaten gehen; \nan dieser Stelle sei darauf hingewiesen, dass die Sicherung von Daten am besten auch mit einer Versionierung und mit einer Dokumentation einhergeht.\n**Datenversionierung** hat den Vorteil, dass Schritte wieder rückgängig gemacht werden können, Datensätze in unterschiedlichen Stadien gespeichert und für eine spätere Weiterarbeit genutzt werden können und einzelne Schritte einzelnen Projektmitarbeiter:innen zugeschrieben werden können.\nZusätzliche Versionierung geht dabei über die Funktionalitäten von Backup-Programmen oder Cloudspeichern wie Dropbox oder Switchdrive hinaus, und für Einzelprojekte wie auch für kollaboratives Arbeiten hat sich in der Wissenschaft wie in der Wirtschaft [git](https://de.wikipedia.org/wiki/Git) etabliert, häufig in Kombination mit Daten-/Coderepositorien auf [GitHub](https://de.wikipedia.org/wiki/GitHub).\nDie meisten von Ihnen werden vermutlich keine eigenen GitHub-Repositorien anlegen, aber das System dennoch irgendwann nutzen, am ehesten durch den Download von dort zur Verfügung gestellten Daten -- die Textdaten für diesen Guide liegen auch in einem [GitHub-Repositorium](https://github.com/wissen-ist-acht/digitalhistory.intro). \nDie **Dokumentation** von gespeicherten Daten schließlich beinhaltet Informationen zur Entstehung des Datensatzes: Wie und von wem wurden die Daten erhoben? Wie wurden sie annotiert? In welchem Format sind die Daten vorhanden? Welche Software wurde an welcher Stelle benutzt? Was stellen die Daten dar?\nDie Sicherung von Daten an mehreren Orten, bspw. auf der lokalen Festplatte, in einem Cloudspeicher und auf einem USB-Stick, schützt sicher vor Datenverlust.\nEine Dokumentation und die Sicherung in einem Repositorium, einem Langzeitspeicher für Daten, sorgt zusätzlich für Sichtbarkeit und die Möglichkeit zur Nachnutzung von Ergebnissen. Als Fachrepositorien für die Geisteswissenschaften existieren beispielsweise [DARIAH-DE](https://de.dariah.eu/repository) oder das [DaSCH](https://www.dasch.swiss/), es gibt spezialisiertere Repositorien wie [AMAD](https://www.amad.org/) (Mittelalter), oder für alle Disziplinen offene wie [Zenodo](https://zenodo.org/) (fächerübergreifend, betrieben durch das CERN). \nSie können Ihre Forschungsdaten dort kostenfrei ablegen, Ihre Urheberschaft nachweisen und die Daten/Publikation mit einem Digital Object Identifier (DOI), also einem eindeutigen und dauerhaften digitalen Identifikator, nachhaltig zitierbar machen.\n\n","srcMarkdownNoYaml":"\n\nJede Art von Forschung ist auf Daten angewiesen, seien sie mittels Personenbefragungen, medizinischer Messungen, Web Scraping oder interpretierender Analysen von Texten erhoben. \nAuf Grundlage von Daten können Forschungsfragen beantwortet, Thesen aufgestellt, Behauptungen widerlegt, Narrative untermauert werden.\nAnalysen, die sich mit einem kleinen Set von Quellen bzw. Daten befassen, präsentieren Ergebnisse dabei oft in Form von Synthesen, die sich aus einer vorangehenden Interpretation der zugrundeliegenden Dokumente ergeben.\nÜber das Quellenverzeichnis und entsprechende Anmerkungen im Text wird die Grundlage nachvollziehbar;\ndass ein bestimmter Abschnitt, ein Satz oder ein Wort auf eine gewisse Weise ausgelegt werden, wird aber auch durch die jeweiligen Forscher:innen selbst beeinflusst -- \neine Literaturwissenschaftlerin beispielsweise, die über Männerfiguren bei Joanne K. Rowling promoviert hat, wird bei der Diskussion um deren mögliche Autorschaft von *The Cuckoo’s Calling* (siehe @sec-digitaletools) diesen Text anders lesen und andere Argumente dafür oder dagegen aufwerfen als ein langjähriger Harry-Potter-Fan mit viel Leseerfahrung, aber anderer bzw. weniger formaler Ausbildung.\nBeide werden fundierte Aussagen treffen und Begründungen geben können, ob und wieso *The Cuckoo’s Calling* von Rowling verfasst wurde oder nicht;\nbeide werden auf ihre Erfahrung und gründliche Auseinandersetzung mit Rowlings Werk verweisen;\nund beide werden mit einzelnen Sätzen oder Passagen für eine Sichtweise argumentieren, die von einer dritten Person genau gegenteilig genutzt würde.\nDie Datengrundlage ist also dieselbe und nachvollziehbar, die Auswertung bzw. die Auswertungsstrategien hingegen sind es nicht mehr, und somit auch nicht die daraus gewonnenen Ergebnisse, die ja auch wieder Forschungsdaten darstellen.\n\nComputergestützte Analysen haben den Anspruch, in allen Schritten nachvollziehbar zu sein und dadurch auch nachnutzbare Daten zu produzieren:\nNicht nur die Quellengrundlage, also die Erhebung von Daten und die Erstellung eines Datensatzes, sondern auch alle Schritte von der Datenanreicherung und -verfeinerung über die genutzten Methoden bzw. Programme für die Auswertung bis hin zur Sicherung und Aufbewahrung sollen transparent, gut dokumentiert und nachvollziehbar sein.\nZum einen, um die Resultate und die darauf fußenden Aussagen belastbar zu machen;\nzum anderen, um die gewonnenen Daten zur weiteren Nutzung kostenfrei und offen verfügbar zu machen.\nZu den Prinzipien, die bei der Arbeit mit Daten berücksichtigt werden sollten, geht es nochmals in [Kapitel 5](05_FAIR_CARE.qmd). \nAn dieser Stelle stehen die konkreten Arbeitsschritte bei der Datenerhebung und -aufbereitung, der Datenanalyse und -sicherung im Zentrum, die in Digital-History-Projekten häufig vorkommen.\n\n\n## Datenerhebung\n\nEs gibt verschiedene Möglichkeiten, Daten für die historische Forschung zu erheben bzw. zu erstellen, von denen einige im Folgenden kurz angesprochen werden.\n\nFür Zeiträume, in denen Quellen vergleichsweise knapp sind und keine seriellen Daten existieren, bietet sich die **Digitalisierung von Texten** und deren anschließende Analyse an. \nDigitalisierung beinhaltet dabei nicht nur die Transformation von einer physischen Quelle in ein digitales Bild, sondern auch die Anreicherung des Bilds mit Layout und Text: \nErst durch eine Markierung von Bereichen, in denen Text vorkommt, ist es in einem zweiten Schritt möglich, diesen als solchen zu erkennen und damit maschinenlesbar und auswertbar zu machen. \nEine solche Umwandlung vom Bild zum Text ist dabei sowohl für moderne Texte, die als Typoskript vorliegen, als auch für vormoderne Handschriften und Drucke möglich, in lateinischer ebenso wie in arabischer, chinesischer oder japanischer Schrift. \nEs gibt kostenpflichtige Programme wie den [Abbyy FineReader](https://pdf.abbyy.com/), aber auch Open-Source-Tools mit und ohne Graphical User Interface (GUI).\nWeit verbreitet ist [Transkribus](https://transkribus.eu), das viele Funktionalitäten bündelt; \ndie Texterkennung ist ab einer gewissen Menge Seiten allerdings kostenpflichtig, wobei studentische Projekte auf Anfrage unterstützt werden können.\nProgramme, die über die Kommandozeile laufen, gänzlich kostenfrei sind und ebenfalls zahlreiche Funktionalitäten bieten, sind beispielsweise [Kraken](https://kraken.re/master/index.html), [OCR4all](https://www.ocr4all.org/), [OCRopus](https://github.com/ocropus/ocropy/wiki) oder [Calamari](https://github.com/Calamari-OCR/calamari).\n\nZur **Extraktion von Daten** aus digitalen/digitalisierten Texten existieren verschiedene Möglichkeiten mithilfe kleiner Kommandozeilenprogramme (eher mühsam und schwierig zu lesen) oder mit Packages für Programmiersprachen, für die Geisteswissenschaften vor allem R oder Python (siehe dazu auch @sec-digitaletools).\nSo können beispielsweise aus digitalisierten Telefonbüchern Entitäten, also Einheiten, wie Personen, Straßennamen oder Berufe oder aus alten Theaterprogrammheften gespielte Stücke, beteiligte Schauspieler:innen und verantwortliche Regisseurinnen extrahiert und als Datensätze weitergenutzt werden.^[Ein gut nachvollziehbares Tutorial zur Extraktion von Daten aus Telefonbüchern hat [Lindsey Wieck](https://lindseywieck.com/) für einen DH-Kurs an der St. Mary’s University in San Antonio erstellt: [https://lindseywieck.com/fall_2016_sf/gatheringdatatutorial.html](https://lindseywieck.com/fall_2016_sf/gatheringdatatutorial.html). [Derek Miller](http://derek.visualizingbroadway.com/about.html) arbeitet zu Broadway-Vorstellungen, [Visualizing Broadway](https://visualizingbroadway.com/index.html), ein Projekt, das [hier](https://digitalhumanities.fas.harvard.edu/100-years-of-broadway-shows-at-once/) beschrieben wird; [hier](https://www.youtube.com/watch?v=KUTPX2Ohcqs) gibt es dazu ein Video in Vorlesungslänge.]\n\nDer anfängliche Aufwand, der einer automatisierten Datenextraktion vorangeht und die steile Lernkurve bei der Bedienung mancher Programme können abschreckend wirken.\nWenn Sie nur ein Theaterprogramm detaillierter auswerten wollen, sind Sie sicher schneller, wenn Sie die entsprechenden Daten in eine Tabellensoftware abtippen.\nWenn Sie aber einen größeren Quellenbestand zur Verfügung haben, der in sich ähnlich strukturiert ist, wie das bei Telefonbüchern oder einer Serie von Theaterprogrammheften der Fall sein dürfte, macht es kaum einen Unterschied mehr, ob Sie zehn oder hundert Theaterprogramme analysieren möchten.\nZudem können Sie Ihr erstelltes Skript, Ihr kleines Computerprogramm, anderen zur Verfügung stellen oder für ähnlich strukturierte Quellen in einem anderen Projekt nachnutzen.\n\nWenn Sie mit bereits digitalisierten Beständen aus öffentlichen Institutionen wie Galerien, Bibliotheken, Museen oder Archiven arbeiten wollen (sog. [GLAM](https://de.wikipedia.org/wiki/GLAM)s: **G**alleries,  **L**ibraries, **A**rchives, **M**useums), besteht oft die Möglichkeit, Daten über **Schnittstellen** herunterzuladen.^[Unter [openglam.ch](https://openglam.ch) finden sich Informationen zu Schweizer GLAM-Einrichtungen, die offene Daten anbieten.]\nSolche Schnittstellen, engl. [API](https://en.wikipedia.org/wiki/API) (**A**pplication **P**rogramming **I**nterface), ermöglichen eine Kommunikation zwischen zwei Computern, ohne dass hierfür der Umweg über eine graphische Oberfläche nötig ist.\nAnstatt also beispielsweise über die Suchmaske der [Staatlichen Museen zu Berlin](https://smb.museum-digital.de/home?navlang=de) nach Objekten oder Dokumenten mithilfe verschiedener Schlagwörter zu suchen und die Ergebnisse dann einzeln herunterzuladen, kann Ihr Computer mit der Schnittstelle des Museums direkt kommunizieren und mit einfachen Befehlen ganze Ergebnislisten zur Weiterarbeit herunterladen. \nFür solche Abfragen können ein Kommandozeilenprogramm oder Programmiersprachen genutzt werden, die Abfrage besteht dabei im Wesentlichen aus einer Zeile, wie hier in der Programmiersprache R:\n\n`library(jsonlite)` \n\n`cats <- fromJSON(\"https://smb.museum-digital.de/json/objects?&s=katze\")`\n\n> Wenn Sie die Schritte nachvollziehen möchten, können Sie R [hier](https://www.r-project.org/) herunterladen. Wenn Sie das Programm öffnen, müssen Sie zuerst das Paket `jsonlite` installieren:\n>`install.packages(\"jsonlite\")`\\\nMit \"Enter\" wird das Paket installiert.\\ \nDann können Sie die zwei Zeilen oben eintippen und ebenfalls mit \"Enter\" ausführen. Die Ergebnisse Ihrer Suche können Sie sich mit\\\n`cats` + \"Enter\" \\\nanzeigen lassen.\n\nDas Ergebnis der Suchanfrage nach \"katze\" wird in der Variable `cats` gespeichert, und diese kann zur Weiterarbeit in ein Tabellenformat exportiert werden:\n\n`write.csv(cats, \"docs/cats_smb.csv\")`\n\nDie Funktion `write.csv` speichert den Inhalt der Variable `cats` als csv-Datei^[**c**omma **s**eparated **v**alue ist ein Format, in dem einzelne Werte, *values*, über spezifische Trenner, meist *commas*, eindeutig abgrenzbar sind und somit in einem Tabellenformat angezeigt werden können, wobei jeder Wert in einer separaten Zelle steht. Tabellensoftware wie Excel, Google Sheets oder Numbers kann csv-Dateien öffnen.] unter dem Dateipfad \"docs/cats_smb.csv\" auf der Festplatte.\n\n![Beginn der Trefferliste für \"katze\" über die API der Staatlichen Museen zu Berlin](images/cats_smb.png)\n\n>Um Abfragen zu vermeiden, die die Server überlasten, haben die meisten APIs entweder eine Authentifizierung oder eine maximale Trefferanzahl pro Abfrage eingebaut.\nBeim obigen Beispiel erhalten Sie dadurch nicht die gesamte Trefferanzahl (134, aus der Spalte \"total\" ersichtlich), sondern nur die ersten 24 -- diese Einstellungen haben die Entwickler:innen der Schnittstelle gemacht.\nUm dennoch alle Treffer mit einer Abfrage zu erhalten, müssten Sie die Dokumentation der API lesen und die Abfrage etwas modifizieren. \n\nWenn Sie das interessiert, finden Sie Details in der Fußnote.[^longnote]\n\n[^longnote]: Die API aus dem Beispiel ist so konfiguriert, dass bei Abfragen mit Ergebnissen über 24 Treffern nur die ersten 24 ausgegeben werden; \ndas ist etwas ungewöhnlich, aber wir können damit umgehen, indem wir die maximale Trefferausgabe pro Anfrage auf 10 setzen -- diese Zahl ist nicht zu hoch, und wir können gut damit rechnen. \nDer Parameter für die maximale Trefferzahl kann mit `&breitenat=10` eingestellt werden.\nDen Startpunkt der Ausgabe kann man mit dem Parameter `&startwert=` ändern. \nUm also alle Treffer für eine Abfrage zu erhalten, können wir die Ergebnisse in 10er-Schritten abfragen und anschließend zusammenfügen.\nDamit das nicht zu einer copy-paste-Aktion wird, müssen wir etwas ausführlicher formulieren bzw. mehrere Variablen verwenden.\nDas hat aber den Vorteil, dass man auf diese Art dann nach jedem Begriff suchen kann.\\\n``` R\nbase_URL <- \"https://smb.museum-digital.de/json/objects?&s=katze\"\ncats <- fromJSON(base_URL)\nstart <-  0\nbreite <- 10\niterations <- cats$total[1]%/%10 + 1\nendsize <- cats$total[1]-(iterations-1) * 10\ncat_list <- data.frame()\nfor (i in 1:iterations){\n  if(i < iterations){\n    cat_list <- rbind(cat_list, fromJSON(paste(base_URL, \"&gbreitenat=10&startwert=\", start , sep=\"\")))\n  } else {\n    cat_list <- rbind(cat_list,fromJSON(paste(base_URL, \"&gbreitenat=\", \n                                                  endsize, \"&startwert=\", start, sep=\"\")))\n  }\n  start <- start + 10\n  write.csv(cat_list, \"Desktop/cat_list.csv\")\n}\n````\nZuerst machen wir den Code übersichtlicher und speichern den Großteil der URL in `base_URL`:\\\n`base_URL <- \"https://smb.museum-digital.de/json/objects?&s=katze\"`\\\nDie Ergebnisse der Suchanfrage werden wieder im Objekt `cats` gespeichert:\\\n`cats <- fromJSON(base_URL)`\\\nDie Anzahl Durchgänge für eine Abfrage ergibt sich aus der `Anzahl der totalen Treffer/10 + 1`; die Anzahl der Treffer lässt sich aus der Spalte \"total\" im Objekt `cats` entnehmen. In R formuliert man das so:\\\n`cats$total[1]`\\\nIm Katzenbeispiel sind es 134 Gesamttreffer, also `(134/10 ohne Rest)+1`, also `14` Durchgänge:\\\n`iterations <- cats$total[1]%/%10 + 1`\\\nDann setzt man den Startwert auf 0:\\\n`start <-  0`\\\nUnd die Maximaltreffer auf 10:\\\n`breite <- 10`\\\nDie letzte Iteration muss dabei nicht die nächsten 10 Treffer abfragen, sondern nur noch 4 (die letzten 4 nach 130):\\\n`endsize <- cats$total[1]-(iterations-1) * 10`\\\nDann erstellen wir eine leere Tabelle, einen data frame, die wir mit unseren Anfragen nach und nach befüllen. (Bei kleinen Datenmengen kann die Funktion `rbind` zur Verbindung von Einzeltabellen genutzt werden; bei größeren Datenmengen ist das iterative Verlängern von data frames nicht empfohlen):\\\n`cat_list <- data.frame()`\\\nWenn wir diese Variablen festgelegt haben, können wir einen Loop, eine Schleife bauen, die unter bestimmten Bedingungen verschiedene Aktionen ausführt:\\\n`for (i in 1:iterations){`\\\nFalls die letzte Iteration noch nicht erreicht ist, wird die Abfrage in 10er-Schritten durchgeführt, wobei der Startwert bei jedem Durchgang um 10 verschoben wird und die Ergebnisse hintereinander in `cat_list` geschrieben werden.\\\n  `if(i < iterations){`\\\n    `cat_list <- rbind(cat_list, fromJSON(paste(base_URL, \"&gbreitenat=10&startwert=\", start , sep=\"\")))`\\\n  ` } else {`\\\n  Sobald die letzte Iteration erreicht ist, werden nicht mehr die nächsten 10, sondern so viel Treffer, wie in `endsize` gespeichert, abgefragt, in unserem Beispiel 4:\\\n  `cat_list <- rbind(cat_list,fromJSON(paste(base_URL, \"&gbreitenat=\",`\\ \n                                                  `endsize, \"&startwert=\", start, sep=\"\")))`\\\n  `}`\\\n  `start <- start + 10`\\\n  Zum Schluss, in diesem Fall nach 14 Iterationen, wird die Tabelle in eine Datei geschrieben:\\\n  `write.csv(cat_list, \"Desktop/cat_list.csv\")`\\\n`}`\n\nWenn Webseiten keine Schnittstellen zur Verfügung stellen, besteht die Möglichkeit, mit **Web Scraping** an gewünschte Daten zu kommen. Je nach Webseite bzw. Inhalten ist die Rechtslage allerdings nicht ganz klar.\nZum Download von Webseiten mit der Programmiersprache Python gibt es [eine Lektion im Programming Historian](https://programminghistorian.org/en/lessons/working-with-web-pages) von William J. Turkel und Adam Crymble.\nEin weiteres Tutorial zur Datenakquise, von Zach Coble, Liz Rodrigues, Erin Pappas, Chelcie Rowell, und Yasmeen Shorish, findet sich [hier](https://dlfteach.pubpub.org/pub/collecting-data-web-scraping/release/1).\n\n## Datenaufbereitung^[Eine häufige Aussage ist, zur Datenvorbereitung/Preprocessing würde 80% der Arbeitszeit verwendet, zur Analyse und Interpretation blieben nur 20%. In einem [Blogartikel](https://blog.ldodds.com/2020/01/31/do-data-scientists-spend-80-of-their-time-cleaning-data-turns-out-no/) von 2020 geht Leigh Dodds diesen Zahlen nach -- ganz so dramatisch ist das Verhältnis in Wahrheit wohl nicht.]\n\nBei der Arbeit mit Datensätzen, seien sie selbst erhoben oder von Dritten übernommen, ist es häufig der Fall, dass Informationen fehlen oder uneinheitlich erhoben wurden, was eine spätere Analyse erschwert.\n\nWenn in einer Umfrage unter Studierenden das Studienfach mit in eine Tabelle aufgenommen wurde, ohne zuvor Werte für diese Kategorie zu definieren, finden sich für \"Geschichte\" und \"Deutsch\" vielleicht auch folgende Varianten:\n\"Gesch.\", \"Geschichtswissenschaft\", \"Geschichtswissenschaften\", \"Geschihcte\", \"Germanistik\", \"Dt.\", \"Germ.\". \nAnstatt zwei Werten für zwei Studienfächer gibt es neun -- ohne, dass sich das Fächerspektrum erweitert hätte. \nIm besten Fall werden solche Varianten schon bei der Erhebung der Daten vermieden, indem eine feste Liste an Werten erstellt wird.\nErhält man jedoch einen Datensatz mit verschiedenen Varianten für ein und denselben Wert, muss man diese zusammenführen, um eine saubere Datengrundlage zu erhalten.\nSie können entweder mit `Strg-R` versuchen, verschiedene Schreibweisen zu finden und zu ersetzen; \nin Tabellenprogrammen wie Excel, Open Office oder Google Sheets können Sie sich einzigartige Werte einzelner Spalten anzeigen lassen und zusammengehörende Varianten zu einem Grundwert zusammenführen;\nam hilfreichsten, recht voraussetzungslos zu bedienen und dabei auch für große Datensätze nutzbar ist die Software [OpenRefine](https://openrefine.org/),  mit der Sie Daten extrahieren,^[Evan Peter Williamson: Fetching and Parsing Data from the Web with OpenRefine, Programming Historian 6 (2017), [https://doi.org/10.46430/phen0065]((https://doi.org/10.46430/phen0065.https://programminghistorian.org/en/lessons/fetch-and-parse-data-with-openrefine)).] säubern/vereinheitlichen^[Seth van Hooland, Ruben Verborgh, Max De Wilde: Cleaning Data with OpenRefine, Programming Historian 2 (2013), [https://doi.org/10.46430/phen0023](https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine).] und anreichern^[Karen Li-Lun Hwang: Enriching Reconciled Data with OpenRefine, The Bytegeist Blog 2018, [https://medium.com/the-bytegeist-blog/enriching-reconciled-data-with-openrefine-89b885dcadbb](https://medium.com/the-bytegeist-blog/enriching-reconciled-data-with-openrefine-89b885dcadbb)] können, um eine für Ihre Forschungsfrage und dafür notwendige Analysen sinnvolle Datengrundlage zu erhalten.\n\nFür Textdaten sind verschiedene Schritte zur Aufbereitung notwendig, je nachdem, welche Methode bzw. Software Sie nutzen möchten. \nFür die meisten Analysen ist es sinnvoll, mit sogenannten Stopword-Listen zu arbeiten.\n[Stopwords](https://en.wikipedia.org/wiki/Stop_word) sind Wörter, die vor einer Analyse aus einem Korpus entfernt werden, um aussagekräftigere Ergebnisse zu erhalten, gerade, wenn es um rein quantitative Methoden zur inhaltlichen Erschließung geht.\nStopwords sind Wörter mit grammatikalischen Funktionen, die in großer Zahl in Dokumenten vorkommen, jedoch wenig Bedeutung tragen.\nWenn man den unbearbeiteten Text dieses Guides nach Worthäufigkeiten auswertet, hier mit [Voyant-Tools](https://voyant-tools.org/) lässt sich nur schwerlich erahnen, worum es geht -- \"digital\" steht auf Platz 12, viel häufiger sind Artikel und Präpositionen.\nMit Hilfe einer Stopword-Liste, die die häufigsten nicht-sinntragenden Wörter aus dem Text entfernt, wird der Inhalt klarer: \n\n::: {layout=\"[46,-8,46]\"}\n![Worthäufigkeiten roher Text](images/word_frequencies_raw.png)\n\n![Worthäufigkeiten ohne Stopwords](images/word_frequencies_stopwords.png)\n:::\n\nWeitere Schritte beinhalten oft eine [Tokenisierung](https://de.wikipedia.org/wiki/Tokenisierung), also die Segmentierung in Einheiten der Wortebene, und eine [Lemmatisierung](https://de.wikipedia.org/wiki/Lemma_(Lexikographie)#Lemmatisierung), also die Rückführung von verschiedenene Formen eines Worts auf eine Grundform -- aus \"ist\", \"war\" und \"sind\" wird \"sein\".\nWie bei den Schreibvarianten der Studienfächer haben die verschiedenen Flexionsformen für die meisten Forschungsfragen keinen Mehrwert und können zur weiteren Analyse zusammengeführt werden.\nFür solche vorbereitenden Schritte gibt es existierende Software und Packages für Programmiersprachen, sodass hier das Rad nicht neu erfunden werden muss, vor allem für moderne, weit verbreitete Sprachen, siehe auch @sec-digitaltools.\nSchwieriger wird es für nicht-standardisierte Sprachen bzw. Sprachformen, also dialektal geprägte oder vormoderne Texte. \nZwar gibt es auch hierfür Programme, die tatsächlich erreichte Präzision muss dabei jedoch je nach Quelle beurteilt werden.\n\n\n## Datenanalyse\n\nWenn Sie einen Datensatz zur Analyse zur Verfügung haben, aus selbst erhobenen Daten oder durch Nachnutzung eines vorhandenen, und für Ihre Zwecke aufbereitet haben, folgt (endlich) auch die Analyse.\nWelche Software oder Methoden Sie verwenden, hängt dabei nicht nur von der Art und Menge der Daten, sondern auch dem Datenformat und vor allem auch Ihrer Forschungsfrage ab.\nWenn Sie eine Personendatenbank haben, in der Briefschreiber:innen und Empfänger:innen aufgenommen sind und der Wohnort der Personen bekannt ist, Sie es jedoch versäumt haben, die Datierungen der Einzelbriefe zu verzeichnen, können Sie nur eine räumliche Verteilung, keine raum-zeitliche Entwicklung eines Briefschreiber:innennetzwerks darstellen.^[Ein Großprojekt an der Universität Stanford, [\"Mapping the Republic of Letters\"](http://republicofletters.stanford.edu/), hat für das 18. Jahrhundert das Briefnetzwerk europäischer Gelehrter modelliert. Ein Fallbeispiel ist das Netzwerk Voltaires, in verschiedenen Visualisierungen: [http://republicofletters.stanford.edu/publications/voltaire/letters/](http://republicofletters.stanford.edu/publications/voltaire/letters/). Dan Edelstein. Interactive Visualization for Voltaire’s Correspondence Network. Letters in Voltaire’s Network [Created using Palladio, http://hdlab.stanford.edu/palladio].] \nWenn Sie aber nur an der örtlichen Verteilung weiblicher und männlicher Verfasser:innen interessiert sind und die zeitliche Komponente für Sie keine Rolle spielt, erübrigt sich auch ein raum-zeitliche Analyse.\nBevor Sie sich also für eine Methode entscheiden, sollten Sie sich fragen, zu welchem Zweck Sie Ihren Datensatz nutzen wollen und  welche Frage(n) er beantworten soll. \n\nIn einem nächsten Schritt sollte über die konkrete Art der Analyse nachgedacht werden, die mit den vorhandenen Daten möglich ist.\nUnter den zahlreichen Möglichkeiten für die Arbeit mit **strukturellen Daten** sind für die Geschichtswissenschaften u.a. die [Netzwerkanalyse](https://de.wikipedia.org/wiki/Soziale_Netzwerkanalyse) oder die [Regressionsanalyse](https://de.wikipedia.org/wiki/Regressionsanalyse) häufig genutzte Methoden. \nFür **textuelle Daten** bieten sich ebenfalls verschiedene Arten der Analyse an, darunter beispielsweise Auszählungen von Worthäufigkeiten als Teil der [Stylometrie](https://de.wikipedia.org/wiki/Stilometrie)/Zuschreibung von Autor:innenschaft (siehe @sec-digitaletools), [Topic Modelling](https://en.wikipedia.org/wiki/Topic_model) als statistische Methode zur Identifizierung wiederkehrender Themen in größeren Textbeständen, oder [Sentimentanalyse](https://en.wikipedia.org/wiki/Sentiment_analysis), um Stimmungen, Gefühle, Bewertungen aus Textpassagen zu extrahieren.\nWenn Sie über **georeferenzierte Daten** verfügen, können Sie verschiedene Analysen mithilfe von [GIS](https://guides.temple.edu/gisfords) (Geographic Information System) durchführen und visualisieren.\n\nOb Sie für Topic Modelling ein eigenes Skript schreiben oder vorhandene Software nutzen, ob Sie Regressionsanalysen selbst durchführen oder auf Webseiten durchführen lassen, ist dabei Ihre Entscheidung;\noftmals ist das Nutzen vorhandener Webangebote für erste kurze Analysen sinnvoll, um zu überlegen, ob die vorgesehene Methode überhaupt sinnvolle Ergebnisse  liefern kann.\nFür größere Projekte, in denen komplexere Analysen über einen längeren Zeitraum durchgeführt werden sollen, bietet sich die Arbeit mit Programmiersprachen schon allein deswegen an, weil so ein sehr hohes Maß an Anpassungen von vorhandenen Funktionen für die eigenen Zwecke und die völlige Kontrolle über die eigenen Daten ermöglicht wird.\nEine Auflistung häufig genutzter Tools für die historische Arbeit findet sich in @sec-digitaltools.\n\n\n## Datensicherung {#sec-sicherung}\n\nIn [Kapitel 5](05_FAIR_CARE.qmd) wird es um Fragen zur nachhaltigen Speicherung von Forschungsdaten gehen; \nan dieser Stelle sei darauf hingewiesen, dass die Sicherung von Daten am besten auch mit einer Versionierung und mit einer Dokumentation einhergeht.\n**Datenversionierung** hat den Vorteil, dass Schritte wieder rückgängig gemacht werden können, Datensätze in unterschiedlichen Stadien gespeichert und für eine spätere Weiterarbeit genutzt werden können und einzelne Schritte einzelnen Projektmitarbeiter:innen zugeschrieben werden können.\nZusätzliche Versionierung geht dabei über die Funktionalitäten von Backup-Programmen oder Cloudspeichern wie Dropbox oder Switchdrive hinaus, und für Einzelprojekte wie auch für kollaboratives Arbeiten hat sich in der Wissenschaft wie in der Wirtschaft [git](https://de.wikipedia.org/wiki/Git) etabliert, häufig in Kombination mit Daten-/Coderepositorien auf [GitHub](https://de.wikipedia.org/wiki/GitHub).\nDie meisten von Ihnen werden vermutlich keine eigenen GitHub-Repositorien anlegen, aber das System dennoch irgendwann nutzen, am ehesten durch den Download von dort zur Verfügung gestellten Daten -- die Textdaten für diesen Guide liegen auch in einem [GitHub-Repositorium](https://github.com/wissen-ist-acht/digitalhistory.intro). \nDie **Dokumentation** von gespeicherten Daten schließlich beinhaltet Informationen zur Entstehung des Datensatzes: Wie und von wem wurden die Daten erhoben? Wie wurden sie annotiert? In welchem Format sind die Daten vorhanden? Welche Software wurde an welcher Stelle benutzt? Was stellen die Daten dar?\nDie Sicherung von Daten an mehreren Orten, bspw. auf der lokalen Festplatte, in einem Cloudspeicher und auf einem USB-Stick, schützt sicher vor Datenverlust.\nEine Dokumentation und die Sicherung in einem Repositorium, einem Langzeitspeicher für Daten, sorgt zusätzlich für Sichtbarkeit und die Möglichkeit zur Nachnutzung von Ergebnissen. Als Fachrepositorien für die Geisteswissenschaften existieren beispielsweise [DARIAH-DE](https://de.dariah.eu/repository) oder das [DaSCH](https://www.dasch.swiss/), es gibt spezialisiertere Repositorien wie [AMAD](https://www.amad.org/) (Mittelalter), oder für alle Disziplinen offene wie [Zenodo](https://zenodo.org/) (fächerübergreifend, betrieben durch das CERN). \nSie können Ihre Forschungsdaten dort kostenfrei ablegen, Ihre Urheberschaft nachweisen und die Daten/Publikation mit einem Digital Object Identifier (DOI), also einem eindeutigen und dauerhaften digitalen Identifikator, nachhaltig zitierbar machen.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"04_datenerhebung_analyse.html"},"language":{"toc-title-document":"Inhaltsverzeichnis","toc-title-website":"Auf dieser Seite","related-formats-title":"Andere Formate","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Quelle","section-title-abstract":"Zusammenfassung","section-title-appendices":"Anhang","section-title-footnotes":"Fußnoten","section-title-references":"Literatur","section-title-reuse":"Wiederverwendung","section-title-copyright":"Urheberrechte","section-title-citation":"Zitat","appendix-attribution-cite-as":"Bitte zitieren Sie diese Arbeit als:","appendix-attribution-bibtex":"Mit BibTeX zitieren:","title-block-author-single":"Autor:in","title-block-author-plural":"Autor:innen","title-block-affiliation-single":"Zugehörigkeit","title-block-affiliation-plural":"Zugehörigkeiten","title-block-published":"Veröffentlichungsdatum","title-block-modified":"Geändert","callout-tip-title":"Tipp","callout-note-title":"Hinweis","callout-warning-title":"Warnung","callout-important-title":"Wichtig","callout-caution-title":"Vorsicht","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Gesamten Code zeigen","code-tools-hide-all-code":"Gesamten Code verbergen","code-tools-view-source":"Quellcode anzeigen","code-tools-source-code":"Quellcode","code-line":"Zeile","code-lines":"Zeilen","copy-button-tooltip":"In die Zwischenablage kopieren","copy-button-tooltip-success":"Kopiert","repo-action-links-edit":"Seite editieren","repo-action-links-source":"Quellcode anzeigen","repo-action-links-issue":"Problem melden","back-to-top":"Zurück nach oben","search-no-results-text":"Keine Treffer","search-matching-documents-text":"Treffer","search-copy-link-title":"Link in die Suche kopieren","search-hide-matches-text":"Zusätzliche Treffer verbergen","search-more-match-text":"weitere Treffer in diesem Dokument","search-more-matches-text":"weitere Treffer in diesem Dokument","search-clear-button-title":"Zurücksetzen","search-detached-cancel-button-title":"Abbrechen","search-submit-button-title":"Abschicken","search":"Suchen","toggle-section":"Abschnitt umschalten","toggle-sidebar":"Seitenleiste umschalten","toggle-dark-mode":"Dunkelmodus umschalten","toggle-reader-mode":"Lesemodus umschalten","toggle-navigation":"Navigation umschalten","crossref-fig-title":"Abbildung","crossref-tbl-title":"Tabelle","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Korollar","crossref-prp-title":"Aussage","crossref-cnj-title":"Annahme","crossref-def-title":"Definition","crossref-exm-title":"Beispiel","crossref-exr-title":"Übungsaufgabe","crossref-ch-prefix":"Kapitel","crossref-apx-prefix":"Anhang","crossref-sec-prefix":"Kapitel","crossref-eq-prefix":"Gleichung","crossref-lof-title":"Abbildungsverzeichnis","crossref-lot-title":"Tabellenverzeichnis","crossref-lol-title":"Listingverzeichnis","environment-proof-title":"Beweis","environment-remark-title":"Anmerkung","environment-solution-title":"Lösung","listing-page-order-by":"Sortieren nach","listing-page-order-by-default":"Voreinstellung","listing-page-order-by-date-asc":"Datum (aufsteigend)","listing-page-order-by-date-desc":"Neueste","listing-page-order-by-number-desc":"Absteigend","listing-page-order-by-number-asc":"Aufsteigend","listing-page-field-date":"Datum","listing-page-field-title":"Titel","listing-page-field-description":"Beschreibung","listing-page-field-author":"Autor:in","listing-page-field-filename":"Dateiname","listing-page-field-filemodified":"Geändert","listing-page-field-subtitle":"Untertitel","listing-page-field-readingtime":"Lesezeit","listing-page-field-categories":"Kategorien","listing-page-minutes-compact":"{0} min","listing-page-category-all":"alle","listing-page-no-matches":"Keine Treffer"},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"99.9.9","bibliography":["references.bib"],"csl":"infoclio-de.csl","theme":["cosmo"],"title":"Datenerhebung, -aufbereitung und -analyse","suppress-bibliography":true,"editor":{"markdown":{"wrap":"sentence"}}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","cite-method":"biblatex","output-file":"04_datenerhebung_analyse.pdf"},"language":{"toc-title-document":"Inhaltsverzeichnis","toc-title-website":"Auf dieser Seite","related-formats-title":"Andere Formate","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Quelle","section-title-abstract":"Zusammenfassung","section-title-appendices":"Anhang","section-title-footnotes":"Fußnoten","section-title-references":"Literatur","section-title-reuse":"Wiederverwendung","section-title-copyright":"Urheberrechte","section-title-citation":"Zitat","appendix-attribution-cite-as":"Bitte zitieren Sie diese Arbeit als:","appendix-attribution-bibtex":"Mit BibTeX zitieren:","title-block-author-single":"Autor:in","title-block-author-plural":"Autor:innen","title-block-affiliation-single":"Zugehörigkeit","title-block-affiliation-plural":"Zugehörigkeiten","title-block-published":"Veröffentlichungsdatum","title-block-modified":"Geändert","callout-tip-title":"Tipp","callout-note-title":"Hinweis","callout-warning-title":"Warnung","callout-important-title":"Wichtig","callout-caution-title":"Vorsicht","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Gesamten Code zeigen","code-tools-hide-all-code":"Gesamten Code verbergen","code-tools-view-source":"Quellcode anzeigen","code-tools-source-code":"Quellcode","code-line":"Zeile","code-lines":"Zeilen","copy-button-tooltip":"In die Zwischenablage kopieren","copy-button-tooltip-success":"Kopiert","repo-action-links-edit":"Seite editieren","repo-action-links-source":"Quellcode anzeigen","repo-action-links-issue":"Problem melden","back-to-top":"Zurück nach oben","search-no-results-text":"Keine Treffer","search-matching-documents-text":"Treffer","search-copy-link-title":"Link in die Suche kopieren","search-hide-matches-text":"Zusätzliche Treffer verbergen","search-more-match-text":"weitere Treffer in diesem Dokument","search-more-matches-text":"weitere Treffer in diesem Dokument","search-clear-button-title":"Zurücksetzen","search-detached-cancel-button-title":"Abbrechen","search-submit-button-title":"Abschicken","search":"Suchen","toggle-section":"Abschnitt umschalten","toggle-sidebar":"Seitenleiste umschalten","toggle-dark-mode":"Dunkelmodus umschalten","toggle-reader-mode":"Lesemodus umschalten","toggle-navigation":"Navigation umschalten","crossref-fig-title":"Abbildung","crossref-tbl-title":"Tabelle","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Korollar","crossref-prp-title":"Aussage","crossref-cnj-title":"Annahme","crossref-def-title":"Definition","crossref-exm-title":"Beispiel","crossref-exr-title":"Übungsaufgabe","crossref-ch-prefix":"Kapitel","crossref-apx-prefix":"Anhang","crossref-sec-prefix":"Kapitel","crossref-eq-prefix":"Gleichung","crossref-lof-title":"Abbildungsverzeichnis","crossref-lot-title":"Tabellenverzeichnis","crossref-lol-title":"Listingverzeichnis","environment-proof-title":"Beweis","environment-remark-title":"Anmerkung","environment-solution-title":"Lösung","listing-page-order-by":"Sortieren nach","listing-page-order-by-default":"Voreinstellung","listing-page-order-by-date-asc":"Datum (aufsteigend)","listing-page-order-by-date-desc":"Neueste","listing-page-order-by-number-desc":"Absteigend","listing-page-order-by-number-asc":"Aufsteigend","listing-page-field-date":"Datum","listing-page-field-title":"Titel","listing-page-field-description":"Beschreibung","listing-page-field-author":"Autor:in","listing-page-field-filename":"Dateiname","listing-page-field-filemodified":"Geändert","listing-page-field-subtitle":"Untertitel","listing-page-field-readingtime":"Lesezeit","listing-page-field-categories":"Kategorien","listing-page-minutes-compact":"{0} min","listing-page-category-all":"alle","listing-page-no-matches":"Keine Treffer"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"csl":"infoclio-de.csl","documentclass":"scrreprt","date":"last-modified","date-format":"[Last compiled on] DD.MM.YYYY","subtitle":"https://wissen-ist-acht.github.io/digitalhistory.intro/","biblio-title":"References","title":"Datenerhebung, -aufbereitung und -analyse","suppress-bibliography":true,"lang":"de","editor":{"markdown":{"wrap":"sentence"}}},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}